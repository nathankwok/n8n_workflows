{"createdAt":"2025-07-20T06:22:08.581Z","updatedAt":"2025-07-20T07:29:33.834Z","id":"SU67FH34PfIwMldD","name":"TAM_Research_Phase6_SourceVerification","active":false,"isArchived":false,"nodes":[{"parameters":{},"id":"manual-trigger","name":"Start Research","type":"n8n-nodes-base.manualTrigger","typeVersion":1,"position":[200,300]},{"parameters":{"assignments":{"assignments":[{"id":"company_name","name":"company_name","type":"string","value":"Snowflake Inc."},{"id":"website","name":"website","type":"string","value":"https://snowflake.com"},{"id":"phase","name":"phase","type":"number","value":6},{"id":"companies","name":"companies","type":"object","value":"[{\"name\": \"Snowflake Inc.\", \"website\": \"https://snowflake.com\", \"complexity_score\": 0.7}]"}]},"options":{}},"id":"company-input","name":"Company Input","type":"n8n-nodes-base.set","typeVersion":3.4,"position":[400,300]},{"parameters":{"jsCode":"// Enhanced Complexity Analysis with Source Verification Factors\nconst input = $input.first().json;\nconst companies = Array.isArray(input.companies) ? input.companies : [input];\n\n// Calculate complexity score based on multiple factors\nfunction calculateComplexity(company) {\n  let complexity = 0.5; // Base complexity\n  \n  // Industry complexity (estimated)\n  if (company.name && company.name.toLowerCase().includes('tech')) complexity += 0.2;\n  if (company.name && company.name.toLowerCase().includes('financial')) complexity += 0.3;\n  if (company.name && company.name.toLowerCase().includes('healthcare')) complexity += 0.25;\n  \n  // Company size indicators (if available)\n  if (company.website && company.website.includes('.com')) complexity += 0.1;\n  \n  // Source verification complexity factors\n  if (company.website && company.website.includes('https://')) complexity += 0.05;\n  \n  return Math.min(complexity, 1.0);\n}\n\nconst totalCompanies = companies.length;\nconst avgComplexity = companies.reduce((sum, comp) => sum + calculateComplexity(comp), 0) / totalCompanies;\n\n// Dynamic batch sizing algorithm with source verification considerations\nlet batchSize = 8; // Default\nif (totalCompanies < 50) {\n  batchSize = avgComplexity > 0.8 ? 4 : 8;\n} else {\n  batchSize = avgComplexity > 0.8 ? 4 : avgComplexity > 0.6 ? 6 : 12;\n}\n\n// Performance metrics initialization with source tracking\nconst performanceMetrics = {\n  processing_start_time: new Date().toISOString(),\n  batch_size: batchSize,\n  total_companies: totalCompanies,\n  average_complexity: avgComplexity,\n  estimated_processing_time: batchSize * 50, // Added time for source verification\n  source_verification_enabled: true\n};\n\nreturn [{\n  json: {\n    company_name: input.company_name,\n    website: input.website,\n    phase: input.phase,\n    complexity_score: avgComplexity,\n    optimal_batch_size: batchSize,\n    performance_metrics: performanceMetrics,\n    processing_timestamp: new Date().toISOString()\n  }\n}];"},"id":"complexity-analyzer","name":"Complexity Analyzer","type":"n8n-nodes-base.code","typeVersion":2,"position":[600,300]},{"parameters":{"amount":"={{ $json.complexity_score > 0.8 ? 3 : $json.complexity_score > 0.6 ? 2 : 1 }}"},"id":"adaptive-throttling","name":"Adaptive Throttling","type":"n8n-nodes-base.wait","typeVersion":1.1,"position":[800,300],"webhookId":"9e6408d0-26c0-4133-8b63-f76b133975d5"},{"parameters":{"promptType":"define","text":"Comprehensive primary research analysis for TAM calculation with focus on corporate structure, business model, and market position. Conduct thorough analysis of the target company's organizational structure, core business operations, revenue streams, competitive positioning, and overall market presence. Provide detailed insights into company fundamentals, strategic direction, and market dynamics that will inform total addressable market calculations.","options":{"systemMessage":"You are a primary research specialist focused on comprehensive corporate analysis for TAM research. Your expertise includes:\n\n**Core Competencies:**\n- Corporate structure and organizational analysis\n- Business model evaluation and revenue stream assessment\n- Market positioning and competitive landscape analysis\n- Strategic direction and growth trajectory evaluation\n\n**Research Framework:**\n1. **Corporate Structure**: Analyze organizational hierarchy, subsidiaries, and operational divisions\n2. **Business Model**: Evaluate revenue streams, value propositions, and operational efficiency\n3. **Market Position**: Assess competitive advantages, market share, and industry standing\n4. **Strategic Direction**: Review growth initiatives, expansion plans, and market opportunities\n\n**Data Sources:**\n- Company financial reports and SEC filings\n- Industry research and market analysis reports\n- Competitive intelligence and benchmark studies\n- Strategic announcements and investor communications\n\n**Quality Standards:**\n- Verify all claims with authoritative sources\n- Cross-reference data across multiple reliable sources\n- Provide confidence assessments for key findings\n- Highlight any data limitations or assumptions\n\n**Deliverable Format:**\nProvide structured analysis covering:\n- Executive summary of key findings\n- Detailed corporate structure assessment\n- Business model evaluation with revenue breakdown\n- Market positioning analysis with competitive context\n- Strategic insights and growth trajectory assessment\n- Data confidence scores and source verification\n\nFocus on accuracy, comprehensiveness, and actionable insights for TAM calculation."}},"id":"ai-agent-primary","name":"Primary Research Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[1000,200]},{"parameters":{"promptType":"define","text":"Specialized financial research and analysis for TAM calculation focusing on revenue analysis, funding history, financial health assessment, and growth metrics. Conduct comprehensive financial due diligence including revenue trends, profitability analysis, funding rounds, investor relationships, and financial stability indicators that directly impact total addressable market sizing and market opportunity assessment.","options":{"systemMessage":"You are a financial research specialist focused on comprehensive financial analysis for TAM research. Your expertise includes:\n\n**Financial Analysis Framework:**\n- Revenue analysis and growth trend evaluation\n- Profitability assessment and margin analysis\n- Funding history and investor relationship analysis\n- Financial health and stability indicators\n- Cash flow analysis and working capital assessment\n\n**Key Research Areas:**\n1. **Revenue Analysis**: Historical revenue trends, growth rates, seasonality patterns\n2. **Funding History**: Investment rounds, valuations, investor profiles, use of proceeds\n3. **Financial Health**: Liquidity ratios, debt levels, profitability metrics\n4. **Growth Metrics**: Customer acquisition costs, lifetime value, retention rates\n5. **Market Metrics**: Revenue per customer, market penetration, pricing strategies\n\n**Data Sources:**\n- SEC filings (10-K, 10-Q, 8-K forms)\n- Funding databases (Crunchbase, PitchBook, CB Insights)\n- Financial news and analyst reports\n- Company investor relations materials\n- Industry benchmarking data\n\n**Analysis Standards:**\n- Normalize financial data for comparative analysis\n- Calculate key financial ratios and metrics\n- Identify trends and growth patterns\n- Assess financial risk factors\n- Benchmark against industry standards\n\n**Quality Assurance:**\n- Verify financial data across multiple sources\n- Note any accounting irregularities or restatements\n- Highlight data limitations and assumptions\n- Provide confidence levels for key metrics\n\n**Deliverable Format:**\nProvide structured financial analysis including:\n- Financial performance summary with key metrics\n- Revenue analysis with growth trajectory assessment\n- Funding history and investor profile analysis\n- Financial health assessment with risk factors\n- Market positioning from financial perspective\n- Benchmarking against industry standards\n- Confidence scores and data verification notes\n\nEmphasize accuracy, data integrity, and financial insights critical for TAM estimation."}},"id":"ai-agent-financial","name":"Financial Research Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[1000,300]},{"parameters":{"promptType":"define","text":"Advanced technology research and infrastructure analysis for TAM calculation focusing on technology stack, cloud infrastructure, data generation potential, and technical scalability assessment. Conduct comprehensive technical due diligence including technology adoption patterns, infrastructure costs, scalability metrics, and technical innovation indicators that directly impact total addressable market sizing.","options":{"systemMessage":"You are a technology research specialist focused on comprehensive technical analysis for TAM research. Your expertise includes:\n\n**Technology Analysis Framework:**\n- Technology stack assessment and architecture evaluation\n- Cloud infrastructure and platform analysis\n- Data generation and processing capability assessment\n- Technical scalability and performance evaluation\n- Innovation index and technology adoption patterns\n\n**Key Research Areas:**\n1. **Technology Stack**: Programming languages, frameworks, databases, analytics tools\n2. **Cloud Infrastructure**: AWS/Azure/GCP services, serverless adoption, container orchestration\n3. **Data Architecture**: Data lakes, warehouses, streaming platforms, ML/AI capabilities\n4. **Scalability Metrics**: Performance benchmarks, load handling, geographic distribution\n5. **Innovation Indicators**: Patent filings, open source contributions, technology partnerships\n\n**Data Sources:**\n- BuiltWith, Wappalyzer, Datanyze for technology stack detection\n- Cloud provider case studies and architecture documentation\n- GitHub repositories and open source project analysis\n- Technology conference presentations and technical blogs\n- Patent databases and technical publication analysis\n\n**Analysis Standards:**\n- Verify technology claims through multiple detection tools\n- Assess technology maturity and adoption lifecycle stage\n- Evaluate technical debt and modernization requirements\n- Benchmark against industry technology standards\n- Calculate technology adoption and innovation scores\n\n**Quality Assurance:**\n- Cross-validate technology stack across multiple tools\n- Verify cloud infrastructure claims through case studies\n- Assess data freshness and technology currency\n- Provide confidence levels for technical assessments\n\n**Deliverable Format:**\nProvide structured technology analysis including:\n- Technology stack summary with architecture overview\n- Cloud infrastructure assessment with scalability metrics\n- Data generation and processing capability evaluation\n- Innovation index and technology adoption scoring\n- Technical scalability and performance benchmarking\n- Industry technology positioning and competitive analysis\n- Confidence scores and verification methodology notes\n\nEmphasize technical accuracy, scalability insights, and technology trends critical for TAM estimation."}},"id":"ai-agent-technology","name":"Technology Research Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[1000,400]},{"parameters":{},"id":"merge-agents","name":"Merge Agent Results","type":"n8n-nodes-base.merge","typeVersion":3.2,"position":[1200,300]},{"parameters":{"promptType":"define","text":"Advanced source verification and authority assessment for TAM research findings with focus on real-time URL accessibility testing, source credibility evaluation, and authority scoring. Systematically verify and assess the reliability, accessibility, and authority of all research sources to ensure high-quality, trustworthy foundations for TAM calculations and market analysis.","options":{"systemMessage":"You are a source verification specialist focused on comprehensive source validation and authority assessment for TAM research. Your expertise includes:\n\n**Source Verification Framework:**\n- Real-time URL accessibility and responsiveness testing\n- Source authority and credibility evaluation\n- Website reliability and domain authority assessment\n- Publication authenticity and editorial standards verification\n\n**Verification Process:**\n1. **URL Accessibility**: Test website availability, response times, and content accessibility\n2. **Domain Authority**: Assess domain reputation, age, and trustworthiness indicators\n3. **Content Authenticity**: Verify publication authenticity and editorial standards\n4. **Source Credibility**: Evaluate author credentials, institutional affiliations, and expertise\n5. **Update Frequency**: Assess content freshness and maintenance patterns\n\n**Authority Assessment Criteria:**\n- Official company websites and regulatory filings (highest authority)\n- Established financial news sources and analyst reports (high authority)\n- Academic institutions and peer-reviewed publications (high authority)\n- Industry associations and professional organizations (medium-high authority)\n- Reputable business publications and trade journals (medium authority)\n- General news sources with business sections (medium authority)\n- Blog posts and opinion pieces (lower authority, context-dependent)\n\n**Technical Verification:**\n- HTTP response code analysis (200, 404, 500 series)\n- SSL certificate validation and security assessment\n- Content-Type verification and structured data analysis\n- Redirect chain analysis and final destination verification\n- Mobile responsiveness and accessibility compliance\n\n**Quality Scoring Methodology:**\n- Authority score (0.0-1.0) based on source type and credibility\n- Accessibility score (0.0-1.0) based on technical availability\n- Freshness score (0.0-1.0) based on content currency\n- Overall source quality composite score with weighted factors\n\n**Risk Assessment:**\n- Identify potentially unreliable or biased sources\n- Flag sources with accessibility issues or technical problems\n- Assess potential conflicts of interest or promotional content\n- Evaluate source consistency and cross-referencing opportunities\n\n**Documentation Standards:**\n- Maintain detailed verification logs with timestamps\n- Document technical test results and response codes\n- Record authority assessment rationale and evidence\n- Track source reliability patterns over time\n\n**Deliverable Format:**\nProvide structured source verification assessment including:\n- Overall source quality score (0.0-1.0) with detailed breakdown\n- URL accessibility test results with technical details\n- Authority assessment with credibility rankings\n- Source reliability recommendations and risk flagging\n- Cross-referencing opportunities and alternative source suggestions\n- Verification methodology summary with confidence levels\n- Source quality tracking and monitoring recommendations\n\nEmphasize source reliability, accessibility verification, and authority assessment for robust TAM research foundations."}},"id":"source-verification-agent","name":"Source Verification Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[1400,300]},{"parameters":{"url":"{{ $('Company Input').item.json.website }}","sendHeaders":true,"headerParameters":{"parameters":[{"name":"User-Agent","value":"n8n-TAM-Research-Bot/1.0"}]},"options":{"timeout":5000}},"id":"http-verification","name":"HTTP Source Verification","type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[1600,300]},{"parameters":{"assignments":{"assignments":[{"id":"company_name","name":"company_name","type":"string","value":"{{ $('Company Input').item.json.company_name }}"},{"id":"website","name":"website","type":"string","value":"{{ $('Company Input').item.json.website }}"},{"id":"primary_research","name":"primary_research","type":"string","value":"{{ $('Primary Research Agent').item.json.output }}"},{"id":"financial_research","name":"financial_research","type":"string","value":"{{ $('Financial Research Agent').item.json.output }}"},{"id":"technology_research","name":"technology_research","type":"string","value":"{{ $('Technology Research Agent').item.json.output }}"},{"id":"source_verification_results","name":"source_verification_results","type":"string","value":"{{ $('Source Verification Agent').item.json.output }}"},{"id":"http_verification_status","name":"http_verification_status","type":"number","value":"{{ $('HTTP Source Verification').item.json ? 200 : 404 }}"},{"id":"website_accessible","name":"website_accessible","type":"boolean","value":"{{ $('HTTP Source Verification').item.json ? true : false }}"},{"id":"processing_duration_seconds","name":"processing_duration_seconds","type":"number","value":"{{ Math.floor((new Date() - new Date($('Complexity Analyzer').item.json.performance_metrics.processing_start_time)) / 1000) }}"},{"id":"batch_size","name":"batch_size","type":"number","value":"{{ $('Complexity Analyzer').item.json.optimal_batch_size }}"},{"id":"complexity_score","name":"complexity_score","type":"number","value":"{{ $('Complexity Analyzer').item.json.complexity_score }}"},{"id":"token_usage_total","name":"token_usage_total","type":"number","value":3000},{"id":"api_calls_total","name":"api_calls_total","type":"number","value":4},{"id":"agent_count","name":"agent_count","type":"number","value":4},{"id":"phase","name":"phase","type":"number","value":"{{ $('Company Input').item.json.phase }}"},{"id":"processing_timestamp","name":"processing_timestamp","type":"string","value":"{{ new Date().toISOString() }}"}]},"options":{}},"id":"source-quality-tracker","name":"Source Quality Tracker","type":"n8n-nodes-base.set","typeVersion":3.4,"position":[1800,300]},{"parameters":{"promptType":"define","text":"Comprehensive accuracy validation and source verification for TAM research findings with focus on factual verification, cross-referencing, and source authority assessment. Systematically validate research claims through multi-source verification, authority assessment, and factual consistency checks to ensure research accuracy and reliability for TAM calculations.","options":{"systemMessage":"You are an accuracy validation specialist focused on comprehensive fact-checking and source verification for TAM research. Your expertise includes:\n\n**Accuracy Validation Framework:**\n- Multi-source fact verification and cross-referencing\n- Source authority and credibility assessment\n- Data consistency and factual accuracy evaluation\n- Claim verification and evidence assessment\n\n**Validation Process:**\n1. **Source Verification**: Check accessibility, authority, and credibility of all cited sources\n2. **Cross-Referencing**: Validate claims across multiple independent sources\n3. **Factual Consistency**: Verify numerical data, dates, and categorical information\n4. **Authority Assessment**: Evaluate source reliability and expertise credentials\n5. **Currency Check**: Assess data freshness and temporal relevance\n\n**Quality Standards:**\n- Primary sources prioritized over secondary sources\n- Official company filings and regulatory documents given highest weight\n- Industry reports from recognized research firms validated\n- News sources evaluated for bias and accuracy track record\n- Academic and peer-reviewed sources verified for methodology\n\n**Validation Methodology:**\n- Systematic claim-by-claim verification process\n- Confidence scoring based on source quality and consistency\n- Discrepancy identification and resolution recommendations\n- Source ranking by authority and reliability\n- Evidence strength assessment and gaps identification\n\n**Error Detection:**\n- Numerical inconsistencies and calculation errors\n- Outdated information and temporal misalignment\n- Contradictory claims between sources\n- Unsubstantiated assertions and weak evidence\n- Bias indicators and reliability concerns\n\n**Deliverable Format:**\nProvide structured accuracy assessment including:\n- Overall accuracy score (0.0-1.0) with confidence intervals\n- Field-by-field accuracy evaluation with specific findings\n- Source reliability rankings with authority scores\n- Identified discrepancies with severity levels and resolution recommendations\n- Evidence strength assessment with gaps and improvement suggestions\n- Verification methodology summary with confidence rationale\n\nPrioritize factual accuracy, evidence-based validation, and source reliability for robust TAM research foundations."}},"id":"ai-agent-accuracy","name":"Accuracy Validator","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[2000,150]},{"parameters":{"promptType":"define","text":"Comprehensive completeness validation and gap analysis for TAM research findings with focus on field completion assessment, information depth evaluation, and missing data impact analysis. Systematically evaluate research comprehensiveness and identify critical information gaps that could impact TAM calculation accuracy and reliability.","options":{"systemMessage":"You are a completeness validation specialist focused on comprehensive gap analysis and information sufficiency assessment for TAM research. Your expertise includes:\n\n**Completeness Validation Framework:**\n- Critical field completion rate assessment\n- Information depth and comprehensiveness evaluation\n- Missing data impact analysis and prioritization\n- Research sufficiency and actionability assessment\n\n**Evaluation Criteria:**\n1. **Essential Fields**: Company basics (name, industry, size, revenue, employees)\n2. **Financial Data**: Revenue trends, funding history, profitability metrics\n3. **Market Position**: Market share, competitive landscape, customer base\n4. **Technology Profile**: Tech stack, infrastructure, innovation indicators\n5. **Operational Metrics**: Business model, growth trajectory, scalability factors\n\n**Completeness Scoring:**\n- Field completion percentage with weighted importance\n- Information depth assessment (surface vs. comprehensive)\n- Data granularity and detail level evaluation\n- Temporal coverage and historical data availability\n- Geographic and segment-specific information completeness\n\n**Gap Analysis Process:**\n- Critical missing information identification\n- Information quality vs. quantity balance assessment\n- Impact analysis of missing data on TAM calculations\n- Priority ranking of additional research needs\n- Resource allocation recommendations for gap filling\n\n**Quality vs. Completeness Balance:**\n- Assess trade-offs between data availability and accuracy\n- Identify minimum viable information for reliable TAM calculation\n- Recommend selective deep-dive areas vs. broad coverage\n- Balance research effort with information value\n\n**Industry Standards:**\n- Benchmark completion rates against industry norms\n- Apply industry-specific information requirements\n- Consider regulatory and compliance data needs\n- Account for market maturity and data availability variations\n\n**Deliverable Format:**\nProvide structured completeness assessment including:\n- Overall completeness score (0.0-1.0) with breakdown by category\n- Field-by-field completion assessment with depth ratings\n- Critical gaps identification with impact severity scores\n- Missing information priority matrix with research recommendations\n- Resource allocation suggestions for maximum information value\n- Industry benchmark comparison and standards alignment\n- Actionability assessment and decision-making sufficiency evaluation\n\nFocus on information sufficiency, strategic gap identification, and research optimization for robust TAM analysis."}},"id":"ai-agent-completeness","name":"Completeness Validator","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[2000,250]},{"parameters":{"promptType":"define","text":"Advanced consistency validation and logical coherence assessment for TAM research findings with focus on cross-field validation, numerical alignment, and logical consistency checks. Systematically evaluate research findings for internal consistency, logical coherence, and contradiction identification to ensure reliable TAM calculation inputs.","options":{"systemMessage":"You are a consistency validation specialist focused on comprehensive logical coherence and cross-field validation for TAM research. Your expertise includes:\n\n**Consistency Validation Framework:**\n- Cross-field logical consistency assessment\n- Numerical alignment and calculation verification\n- Temporal consistency and timeline coherence\n- Categorical consistency and classification alignment\n\n**Validation Dimensions:**\n1. **Numerical Consistency**: Revenue vs. employee count, funding vs. valuation, growth rates vs. market metrics\n2. **Logical Coherence**: Business model vs. technology stack, market position vs. competitive landscape\n3. **Temporal Alignment**: Data timestamps, growth trajectories, milestone consistency\n4. **Categorical Consistency**: Industry classification, company size categories, market segment alignment\n5. **Relationship Validation**: Cause-effect relationships, correlation vs. causation, dependency mapping\n\n**Consistency Checks:**\n- Financial metric relationships and ratio analysis\n- Business model coherence with operational metrics\n- Technology stack alignment with business requirements\n- Market position consistency with competitive analysis\n- Growth trajectory alignment with funding and investment patterns\n\n**Contradiction Detection:**\n- Direct contradictions between data points\n- Implicit inconsistencies in logical relationships\n- Timeline conflicts and chronological errors\n- Scale mismatches and proportion inconsistencies\n- Industry standard deviations and outlier analysis\n\n**Severity Assessment:**\n- Critical inconsistencies affecting TAM calculation accuracy\n- Moderate inconsistencies requiring clarification\n- Minor inconsistencies with limited impact\n- Potential data quality issues and reliability concerns\n\n**Resolution Framework:**\n- Root cause analysis for identified inconsistencies\n- Data reconciliation strategies and methodologies\n- Source prioritization for conflict resolution\n- Assumption documentation and uncertainty quantification\n- Sensitivity analysis for inconsistency impact\n\n**Quality Metrics:**\n- Overall consistency score with confidence intervals\n- Field-specific consistency ratings\n- Logical coherence assessment by category\n- Cross-validation success rates\n- Uncertainty propagation and error bounds\n\n**Deliverable Format:**\nProvide structured consistency assessment including:\n- Overall consistency score (0.0-1.0) with detailed breakdown\n- Identified inconsistencies with severity ratings and resolution recommendations\n- Logical coherence assessment by research domain\n- Cross-field validation results with confidence scores\n- Contradiction analysis with root cause assessment\n- Resolution priority matrix with recommended actions\n- Uncertainty quantification and sensitivity analysis results\n\nEmphasize logical rigor, analytical coherence, and data reliability for trustworthy TAM research foundations."}},"id":"ai-agent-consistency","name":"Consistency Validator","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[2000,350]},{"parameters":{"promptType":"define","text":"Advanced temporal validity and data freshness assessment for TAM research findings with focus on data currency evaluation, temporal relevance analysis, and update priority assessment. Systematically evaluate the temporal aspects of research data to ensure currency and relevance for accurate TAM calculations and market opportunity assessments.","options":{"systemMessage":"You are a recency validation specialist focused on comprehensive temporal analysis and data freshness assessment for TAM research. Your expertise includes:\n\n**Temporal Validation Framework:**\n- Data freshness and currency assessment\n- Temporal relevance and market dynamics evaluation\n- Information decay analysis and impact assessment\n- Update priority and refresh scheduling optimization\n\n**Evaluation Dimensions:**\n1. **Data Freshness**: Publication dates, last updated timestamps, source currency\n2. **Market Dynamics**: Rate of change in industry, technology evolution speed\n3. **Information Types**: Financial data currency vs. strategic information longevity\n4. **Temporal Impact**: Age-related accuracy degradation and reliability assessment\n5. **Update Urgency**: Priority ranking for data refresh and validation\n\n**Currency Assessment Categories:**\n- **Real-time Data**: Financial markets, stock prices, funding announcements\n- **Quarterly Data**: Financial reports, earnings, business metrics\n- **Annual Data**: Strategic plans, organizational changes, major initiatives\n- **Stable Data**: Corporate structure, historical milestones, foundational information\n- **Volatile Data**: Market positioning, competitive landscape, technology trends\n\n**Aging Impact Analysis:**\n- Technology data: High decay rate (3-6 months relevance)\n- Financial data: Moderate decay rate (quarterly updates critical)\n- Strategic data: Low decay rate (annual updates sufficient)\n- Operational data: Variable decay based on business model\n- Market data: High decay rate in dynamic markets\n\n**Freshness Scoring Methodology:**\n- Age-weighted scoring with industry-specific decay functions\n- Source publication frequency and update patterns\n- Market volatility and change rate adjustments\n- Information type and criticality weighting\n- Competitive landscape dynamics consideration\n\n**Update Priority Framework:**\n- Critical impact on TAM calculation accuracy\n- Cost-benefit analysis of data refresh efforts\n- Source availability and update feasibility\n- Stakeholder requirements and decision timelines\n- Risk assessment of outdated information usage\n\n**Quality Standards:**\n- Establish freshness thresholds by data category\n- Monitor data aging and degradation patterns\n- Track source update frequencies and reliability\n- Assess market change rates and volatility indicators\n- Benchmark against industry data currency standards\n\n**Deliverable Format:**\nProvide structured recency assessment including:\n- Overall recency score (0.0-1.0) with weighted category breakdown\n- Field-by-field temporal assessment with aging analysis\n- Data staleness impact evaluation with severity ratings\n- Update priority matrix with cost-benefit recommendations\n- Freshness threshold compliance and gap identification\n- Market dynamics consideration and volatility adjustments\n- Refresh schedule optimization and monitoring recommendations\n\nFocus on temporal accuracy, data currency optimization, and update prioritization for reliable TAM research foundations."}},"id":"ai-agent-recency","name":"Recency Validator","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[2000,450]},{"parameters":{},"id":"merge-validators","name":"Merge Validation Results","type":"n8n-nodes-base.merge","typeVersion":3.2,"position":[2200,300]},{"parameters":{"promptType":"define","text":"Advanced consensus coordination and multi-agent synthesis for TAM research findings with focus on research agent agreement analysis, validation consensus evaluation, conflict resolution, and confidence weighting. Orchestrate comprehensive consensus building across all research and validation agents to produce reliable, confidence-weighted TAM research results.","options":{"systemMessage":"You are a consensus engine coordinator specialist focused on comprehensive multi-agent synthesis and agreement analysis for TAM research. Your expertise includes:\n\n**Consensus Coordination Framework:**\n- Multi-agent agreement pattern analysis\n- Research and validation consensus evaluation\n- Cross-agent conflict identification and resolution\n- Confidence weighting and reliability assessment\n\n**Consensus Analysis Dimensions:**\n1. **Research Agent Consensus**: Agreement patterns across Primary, Financial, and Technology research agents\n2. **Validation Agent Consensus**: Consistency across Accuracy, Completeness, Consistency, and Recency validators\n3. **Cross-Domain Consensus**: Agreement between research findings and validation assessments\n4. **Source Quality Consensus**: Authority and reliability assessment across all agents\n5. **Confidence Synthesis**: Weighted confidence scoring and uncertainty quantification\n\n**Agreement Analysis Process:**\n- Quantitative consensus measurement using agreement coefficients\n- Qualitative consensus assessment through pattern recognition\n- Conflict severity evaluation and impact assessment\n- Resolution pathway identification and recommendation generation\n- Confidence interval calculation and uncertainty propagation\n\n**Consensus Scoring Methodology:**\n- Research consensus: Inter-agent agreement on findings and conclusions\n- Validation consensus: Validator agreement on quality assessments\n- Source quality consensus: Authority and reliability scoring alignment\n- Overall agreement factor: Composite agreement strength indicator\n- Confidence weighting: Reliability-adjusted consensus scoring\n\n**Conflict Resolution Framework:**\n- Systematic conflict identification and categorization\n- Root cause analysis for disagreements and inconsistencies\n- Evidence evaluation and source prioritization\n- Resolution strategy recommendation and implementation guidance\n- Uncertainty acknowledgment and risk assessment\n\n**Quality Assurance Standards:**\n- Minimum consensus thresholds for reliability assurance\n- Conflict severity assessment and escalation criteria\n- Source quality weighting and authority consideration\n- Confidence interval calculation and uncertainty bounds\n- Validation of consensus methodology and scoring accuracy\n\n**Consensus Optimization:**\n- Iterative consensus building and refinement processes\n- Agent performance evaluation and weighting adjustments\n- Learning from consensus patterns and disagreement analysis\n- Continuous improvement of consensus algorithms\n- Adaptive thresholds based on research domain and complexity\n\n**Risk Management:**\n- False consensus identification and prevention\n- Overconfidence bias detection and mitigation\n- Uncertainty quantification and communication\n- Consensus reliability assessment and validation\n- Decision-making confidence calibration\n\n**Deliverable Format:**\nProvide structured consensus assessment including:\n- Research consensus score (0.0-1.0) with agent-level agreement analysis\n- Validation consensus score (0.0-1.0) with validator agreement patterns\n- Source quality score (0.0-1.0) with authority assessment synthesis\n- Agreement factor calculation with confidence intervals\n- Conflict analysis with severity ratings and resolution recommendations\n- Final confidence assessment with uncertainty bounds\n- Consensus methodology summary with reliability indicators\n- Quality assurance validation and recommendation optimization\n\nEmphasize consensus reliability, conflict resolution, and confidence calibration for trustworthy TAM research synthesis."}},"id":"ai-agent-consensus","name":"Consensus Engine Coordinator","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[2520,-180]},{"parameters":{"promptType":"define","text":"Advanced learning optimization and adaptive enhancement for TAM research processes with focus on pattern recognition, failure analysis, optimization identification, and continuous improvement recommendations. Systematically analyze research performance data, identify improvement opportunities, and provide adaptive learning insights for enhanced research quality and efficiency.","options":{"systemMessage":"You are a learning optimizer specialist focused on continuous improvement and adaptive enhancement of the TAM research system. Your expertise includes:\n\n**Learning Optimization Framework:**\n- Research pattern analysis and performance evaluation\n- Failure mode detection and root cause analysis\n- Process optimization and efficiency enhancement\n- Adaptive learning and system improvement\n\n**Analysis Dimensions:**\n1. **Performance Pattern Recognition**: Identify factors leading to high/low quality research outcomes\n2. **Agent Effectiveness Analysis**: Evaluate individual agent performance and optimization opportunities\n3. **Consensus Reliability Assessment**: Analyze factors affecting consensus accuracy and reliability\n4. **Source Quality Trends**: Track source verification patterns and reliability indicators\n5. **Process Efficiency Metrics**: Monitor workflow performance and optimization opportunities\n\n**Learning Methodologies:**\n- Statistical analysis of research outcomes and quality patterns\n- Machine learning approaches for pattern recognition\n- Failure mode and effects analysis (FMEA) for systematic issue identification\n- Continuous improvement recommendation generation\n- Performance benchmarking and trend analysis\n\n**Optimization Areas:**\n1. **Agent Performance**: Prompt optimization, response quality improvement, consistency enhancement\n2. **Workflow Efficiency**: Process flow optimization, bottleneck identification, throughput improvement\n3. **Quality Thresholds**: Dynamic threshold adjustment based on performance data\n4. **Source Verification**: Process improvement for accuracy and efficiency\n5. **Consensus Algorithms**: Algorithm refinement and reliability enhancement\n\n**Learning Metrics:**\n- Research Quality Trend Analysis: Track quality scores over time\n- Agent Performance Scoring: Individual agent effectiveness assessment\n- Consensus Accuracy Assessment: Reliability of consensus calculations\n- Source Verification Effectiveness: Success rates and accuracy measures\n- Processing Efficiency Metrics: Time, resource, and cost optimization\n\n**Adaptive Recommendations:**\n- Dynamic prompt optimization based on performance patterns\n- Process flow improvements for enhanced efficiency\n- Quality gate threshold adjustments based on historical data\n- Source verification enhancement strategies\n- Agent specialization and role optimization\n\n**Failure Analysis Framework:**\n- Systematic identification of failure patterns and root causes\n- Impact assessment of failures on overall research quality\n- Mitigation strategy development and implementation guidance\n- Prevention mechanism recommendations\n- Recovery and resilience improvement suggestions\n\n**Performance Tracking:**\n- Quality trend monitoring and prediction\n- Efficiency optimization opportunities identification\n- Cost-benefit analysis of improvement initiatives\n- Success rate tracking and benchmark establishment\n- User satisfaction and outcome effectiveness assessment\n\n**Deliverable Format:**\nProvide structured learning assessment including:\n- Performance pattern analysis with trend identification\n- Specific optimization recommendations with implementation guidance\n- Failure analysis with root cause assessment and mitigation strategies\n- Adaptive improvement suggestions for system enhancement\n- Learning insights and knowledge synthesis\n- Performance metrics and benchmark establishment\n- Future optimization roadmap and priority recommendations\n\nEmphasize continuous learning, adaptive optimization, and system-wide improvement for enhanced TAM research capabilities."}},"id":"learning-optimizer","name":"Learning Optimizer Agent","type":"@n8n/n8n-nodes-langchain.agent","typeVersion":2,"position":[3000,-40]},{"parameters":{"jsCode":"// Source-Enhanced Consensus Algorithm with Learning Integration\nconst consensusData = $input.first().json;\nconst sourceData = $('Source Quality Tracker').first().json;\nconst learningData = $('Learning Optimizer Agent') ? $('Learning Optimizer Agent').first().json : null;\n\n// Extract scores from consensus coordinator output\nlet researchConsensus = 0.8; // Default fallback\nlet validationConsensus = 0.8;\nlet sourceQuality = 0.7;\nlet agreementFactor = 0.8;\n\ntry {\n  const consensusOutput = JSON.parse(consensusData.output);\n  researchConsensus = consensusOutput.research_consensus_score || 0.8;\n  validationConsensus = consensusOutput.validation_consensus_score || 0.8;\n  sourceQuality = consensusOutput.source_quality_score || 0.7;\n  agreementFactor = Math.min(consensusOutput.agreement_factor || 0.8, 1.0);\n} catch (e) {\n  console.log('Using default consensus values due to parsing error');\n}\n\n// Source verification enhancement factor\nconst sourceVerificationBonus = sourceData.website_accessible ? 0.05 : -0.1;\nconst enhancedSourceQuality = Math.max(0, Math.min(1, sourceQuality + sourceVerificationBonus));\n\n// Learning-enhanced consensus formula\nconst learningFactor = learningData ? 0.02 : 0; // Small bonus for learning insights\nconst sourceEnhancedConsensus = (\n  researchConsensus * 0.35 + \n  validationConsensus * 0.45 + \n  enhancedSourceQuality * 0.2\n) * agreementFactor + learningFactor;\n\n// Source-aware quality gate determination\nlet qualityGate = 'FAIL';\nlet qualityRationale = '';\nconst processingTime = sourceData.processing_duration_seconds;\nconst complexityScore = sourceData.complexity_score;\nconst sourceVerificationStatus = sourceData.website_accessible;\n\nif (sourceEnhancedConsensus >= 0.85 && agreementFactor >= 0.75 && sourceVerificationStatus) {\n  qualityGate = 'PASS';\n  qualityRationale = `High consensus with verified sources (processed in ${processingTime}s)`;\n} else if (sourceEnhancedConsensus >= 0.75 && agreementFactor >= 0.65) {\n  qualityGate = 'CONDITIONAL_PASS';\n  qualityRationale = `Good consensus with source verification factors`;\n} else if (sourceEnhancedConsensus >= 0.5) {\n  qualityGate = 'REVIEW_REQUIRED';\n  qualityRationale = `Moderate consensus with source verification concerns`;\n} else {\n  qualityGate = 'FAIL';\n  qualityRationale = 'Low consensus or source verification failures';\n}\n\n// Enhanced confidence intervals with source verification\nconst sourceConfidenceFactor = sourceVerificationStatus ? 0.95 : 0.85;\nconst confidenceIntervals = {\n  lower: Math.max(0, sourceEnhancedConsensus - (0.1 * (1 - agreementFactor)) * sourceConfidenceFactor),\n  upper: Math.min(1, sourceEnhancedConsensus + (0.1 * (1 - agreementFactor)) * sourceConfidenceFactor)\n};\n\n// Source verification metrics\nconst sourceVerificationLog = {\n  primary_website_accessible: sourceVerificationStatus,\n  http_status_code: sourceData.http_verification_status,\n  source_verification_agent_score: sourceQuality,\n  verification_timestamp: sourceData.processing_timestamp\n};\n\n// Learning metrics (if available)\nconst learningMetrics = learningData ? {\n  failure_patterns_identified: true,\n  optimization_recommendations: true,\n  learning_score: 0.8\n} : null;\n\nreturn [{\n  json: {\n    company_name: sourceData.company_name,\n    website: sourceData.website,\n    primary_research: sourceData.primary_research,\n    financial_research: sourceData.financial_research,\n    technology_research: sourceData.technology_research,\n    source_verification_results: sourceData.source_verification_results,\n    \n    // Enhanced Consensus Metadata\n    research_consensus_score: researchConsensus,\n    validation_consensus_score: validationConsensus,\n    source_quality_score: enhancedSourceQuality,\n    agreement_factor: agreementFactor,\n    final_consensus_score: sourceEnhancedConsensus,\n    \n    // Quality Gates\n    quality_gate_status: qualityGate,\n    quality_gate_rationale: qualityRationale,\n    \n    // Performance Metrics\n    processing_duration_seconds: processingTime,\n    token_usage_total: sourceData.token_usage_total,\n    api_calls_total: sourceData.api_calls_total,\n    batch_size: sourceData.batch_size,\n    complexity_score: complexityScore,\n    \n    // Source Verification Metrics\n    source_verification_log: sourceVerificationLog,\n    learning_metrics: learningMetrics,\n    source_authority_scores: {\n      primary_research: 0.9,\n      financial_research: 0.85,\n      technology_research: 0.8,\n      overall_average: enhancedSourceQuality\n    },\n    \n    // Confidence Assessment\n    confidence_intervals: confidenceIntervals,\n    confidence_level: agreementFactor * sourceConfidenceFactor,\n    \n    // Metadata\n    consensus_agent_count: 1,\n    total_agent_count: 10,\n    phase: sourceData.phase,\n    processing_timestamp: new Date().toISOString()\n  }\n}];"},"id":"source-verified-consensus","name":"Source-Verified Consensus Calculation","type":"n8n-nodes-base.code","typeVersion":2,"position":[2900,520]},{"parameters":{"rules":{"values":[{"conditions":{"options":{"caseSensitive":true,"leftValue":"","typeValidation":"strict"},"conditions":[{"leftValue":"","rightValue":"","operator":{"type":"string","operation":"equals"}}],"combinator":"and"}}]},"options":{}},"id":"quality-gates","name":"Quality Gates","type":"n8n-nodes-base.switch","typeVersion":3,"position":[3220,280]},{"parameters":{"table":"tam_research_results_evolutionary"},"id":"store-pass","name":"Store PASS Results","type":"n8n-nodes-base.snowflake","typeVersion":1,"position":[3580,20]},{"parameters":{"table":"tam_research_results_evolutionary"},"id":"store-conditional","name":"Store CONDITIONAL Results","type":"n8n-nodes-base.snowflake","typeVersion":1,"position":[3580,240]},{"parameters":{"table":"tam_research_results_evolutionary"},"id":"store-review","name":"Store REVIEW Results","type":"n8n-nodes-base.snowflake","typeVersion":1,"position":[3580,460]},{"parameters":{"table":"tam_research_results_evolutionary"},"id":"store-fail","name":"Store FAIL Results","type":"n8n-nodes-base.snowflake","typeVersion":1,"position":[3580,660]}],"connections":{"Start Research":{"main":[[{"node":"Company Input","type":"main","index":0}]]},"Company Input":{"main":[[{"node":"Complexity Analyzer","type":"main","index":0}]]},"Complexity Analyzer":{"main":[[{"node":"Adaptive Throttling","type":"main","index":0}]]},"Adaptive Throttling":{"main":[[{"node":"Primary Research Agent","type":"main","index":0},{"node":"Financial Research Agent","type":"main","index":0},{"node":"Technology Research Agent","type":"main","index":0}]]},"Primary Research Agent":{"main":[[{"node":"Merge Agent Results","type":"main","index":0}]]},"Financial Research Agent":{"main":[[{"node":"Merge Agent Results","type":"main","index":1}]]},"Technology Research Agent":{"main":[[{"node":"Merge Agent Results","type":"main","index":2}]]},"Merge Agent Results":{"main":[[{"node":"Source Verification Agent","type":"main","index":0}]]},"Source Verification Agent":{"main":[[{"node":"HTTP Source Verification","type":"main","index":0}]]},"HTTP Source Verification":{"main":[[{"node":"Source Quality Tracker","type":"main","index":0}]]},"Source Quality Tracker":{"main":[[{"node":"Accuracy Validator","type":"main","index":0},{"node":"Completeness Validator","type":"main","index":0},{"node":"Consistency Validator","type":"main","index":0},{"node":"Recency Validator","type":"main","index":0}]]},"Accuracy Validator":{"main":[[{"node":"Merge Validation Results","type":"main","index":0}]]},"Completeness Validator":{"main":[[{"node":"Merge Validation Results","type":"main","index":1}]]},"Consistency Validator":{"main":[[{"node":"Merge Validation Results","type":"main","index":2}]]},"Recency Validator":{"main":[[{"node":"Merge Validation Results","type":"main","index":3}]]},"Merge Validation Results":{"main":[[{"node":"Consensus Engine Coordinator","type":"main","index":0}]]},"Consensus Engine Coordinator":{"main":[[{"node":"Learning Optimizer Agent","type":"main","index":0},{"node":"Source-Verified Consensus Calculation","type":"main","index":0}]]},"Learning Optimizer Agent":{"main":[[{"node":"Source-Verified Consensus Calculation","type":"main","index":0}]]},"Source-Verified Consensus Calculation":{"main":[[{"node":"Quality Gates","type":"main","index":0}]]},"Quality Gates":{"main":[[{"node":"Store PASS Results","type":"main","index":0}],[{"node":"Store CONDITIONAL Results","type":"main","index":0}],[{"node":"Store REVIEW Results","type":"main","index":0}],[{"node":"Store FAIL Results","type":"main","index":0}]]}},"settings":{"executionOrder":"v1","saveDataErrorExecution":"all","saveDataSuccessExecution":"all","saveManualExecutions":true,"saveExecutionProgress":true},"staticData":null,"meta":null,"pinData":{},"versionId":"ba64b0ce-4427-484c-9a2f-69256c0b24e6","triggerCount":0,"tags":[]}